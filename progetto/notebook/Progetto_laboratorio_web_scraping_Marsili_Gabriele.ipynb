{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81d43714-5a1c-44ff-b586-6cbf1a8d6107",
   "metadata": {},
   "source": [
    "###### <h1><center> Progetto finale Laboratorio di Web Scraping </h1>\n",
    "<h1><center> Anno Accademico 2023-2024 </h1>\n",
    "<h2><center>  Marsili Gabriele </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988cbc50-2032-4a43-90e6-ddeb96473b6f",
   "metadata": {},
   "source": [
    "## Considerazioni iniziali\n",
    "\n",
    "**-Struttura del codice :**\n",
    "Per mantenere il codice chiaro e facilmente interpretabile ho deciso di strutturarlo nei seguenti moduli : \n",
    "- *Main.py* : file principale da cui avviare l'esecuzione\n",
    "- *Utilities.py* : file contenente alcune strutture e funzioni utilizzate da più moduli\n",
    "- *Scraping* : modulo relativo alla parte di scraping \n",
    "- *Graphic* : modulo contenente le funzioni necessarie per la creazione dei grafici \n",
    "- *Dataset_analysis* : modulo in cui ho inserito le funzioni relative all'analisi dei dati ed alle operazioni sui dataframes\n",
    "\n",
    "**-Organizzazione del notebook :**\n",
    "Il seguente notebook è creato in modo tale da poter rispettare la struttura a moduli del codice, quindi vengono prima introdotti i moduli minori (o secondari) e non dipendenti da altri moduli fino ad arrivare al main (che ovviamente necessita della definizione degli altri moduli per poter funzionare correttamente).\n",
    "\n",
    "**-Path dei files**\n",
    "I files csv dovrebbero esser inseriti nella cartella datasetCSV avente percorso : ./dataset_analysis/datasetCSV con i seguenti nomi: \n",
    "- inputs.csv\n",
    "- map.csv\n",
    "- outputs.csv\n",
    "- transactions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9521c6-984c-434b-a523-f42f5bb4b53b",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "Nel seguente codice definisco un paio di dizionari (SETTINGS e LOG_LEVELS) che verranno utilizzati nei vari moduli ed alcune funzioni utili per la gestione delle date nei dataframes e per il calcolo della dimensione dei chunk (per la lettura dei files CSV).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "919694bb-1659-4346-9216-09629e28b817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "SETTINGS = {\n",
    "    'MAX_THREAD_QUANTITY' : 13,\n",
    "    'ELIGIUS_ANALYSIS_STEPS':7,\n",
    "    'SELENIUM_HEADLESS_MODE' : True\n",
    "}\n",
    "\n",
    "LOG_LEVELS = {\n",
    "    'debug' : True,\n",
    "    'results' : True,\n",
    "    'processing' : True,\n",
    "    'all infos' : True,\n",
    "    'time' : True,\n",
    "    'reduce spam':True\n",
    "}\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Convert timestamp to datetime if not already\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['timestamp']):\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "    return df\n",
    "\n",
    "def unix_to_date(timestamp, date_type = 'datetime'):\n",
    "    \"\"\"\n",
    "    Converte un timestamp Unix in un oggetto datetime / \n",
    "    lo converte in formato gg/mm/aaaa - hh:mm:ss se date_type != datetime.\n",
    "    \n",
    "    @param timestamp (int): Timestamp Unix da convertire.\n",
    "    @param date_type (str) (optional) : tipo di conversione ritornata\n",
    "    \n",
    "    @return :\n",
    "     - datetime.datetime: Oggetto datetime corrispondente al timestamp Unix.\n",
    "        oppure \n",
    "     - str: Data formattata come 'gg/mm/aaaa - hh:mm:ss'.\n",
    "    \"\"\"\n",
    "    if(date_type != 'datetime'):\n",
    "        dt = datetime.datetime.fromtimestamp(timestamp)\n",
    "        return format_datetime(dt)\n",
    "    \n",
    "    return datetime.datetime.fromtimestamp(timestamp)\n",
    "\n",
    "def format_datetime(dt):\n",
    "    \"\"\"\n",
    "    Formatta un oggetto datetime in formato 'gg/mm/aaaa - hh:mm:ss'.\n",
    "    \n",
    "    @param dt (datetime.datetime): Oggetto datetime da formattare.\n",
    "    \n",
    "    @return : str: Data formattata come 'gg/mm/aaaa - hh:mm:ss'.\n",
    "    \"\"\"\n",
    "    return dt.strftime('%d/%m/%Y - %H:%M:%S')\n",
    "\n",
    "def calculate_chunk_size(total_rows, desired_chunk_size):\n",
    "    \"\"\"Calcola il numero dei chunk e la relativa dimenione in base \n",
    "    al totale delle righe del file ed alla dimensione desiderata dei chunk.\n",
    "\n",
    "    @param total_rows : totale delle righe del file\n",
    "    @param desired_chunk_size : dimensione di un chunk \n",
    "    \n",
    "    @return num_chunks, actual_chunk_size : il nuumero dei chunk e la relativa size\n",
    "    \"\"\"\n",
    "    \n",
    "    num_chunks = total_rows // desired_chunk_size\n",
    "    if total_rows % desired_chunk_size != 0:\n",
    "        num_chunks += 1\n",
    "    actual_chunk_size = total_rows // num_chunks\n",
    "    return num_chunks, actual_chunk_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b55a63-db32-4b10-bc53-5e9f4fc479df",
   "metadata": {},
   "source": [
    "## Graphic module \n",
    "\n",
    "**Modulo relativo alla crezione dei grafici, sfrutta plot_creator.py**\n",
    "\n",
    "In plot_creator.py ho inserito le funzioni relative alla creazione dei grafici "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682b4ce7-45f2-4ae4-b2dc-f83e30a7fccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import networkx as nx\n",
    "\n",
    "def plot_fees_vs_network_congestion(df):\n",
    "    # Convert the 'month' column to datetime if it's not already\n",
    "    if df['month'].dtype != 'datetime64[ns]':\n",
    "        df['month'] = pd.to_datetime(df['month'])\n",
    "\n",
    "    # Resample the data to 3-month intervals\n",
    "    df_resampled = df.set_index('month').resample('3ME').sum().reset_index()\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot fees\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Month')\n",
    "    ax1.set_ylabel('Fees (in Satoshi)', color=color)\n",
    "    ax1.plot(df_resampled['month'], df_resampled['fees'], color=color, label='Fees')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n",
    "\n",
    "    # Create a second y-axis for the network congestion\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Network Congestion (bytes)', color=color)\n",
    "    ax2.plot(df_resampled['month'], df_resampled['networkCongestion'], color=color, label='Network Congestion')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n",
    "\n",
    "    ax1.legend(loc='upper left', bbox_to_anchor=(0, 1))  \n",
    "    ax2.legend(loc='upper left', bbox_to_anchor=(0, 0.9))\n",
    "\n",
    "    # Adjust layout to make room for title\n",
    "    plt.tight_layout(rect=[0, 0.1, 1, 0.9])\n",
    "    fig.suptitle('Fees vs Network Congestion Over Time (3-Month Intervals)', y=0.95, fontsize=12)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_script_type_usage(df):\n",
    "    # Convert the 'month' column to datetime if it's not already\n",
    "    if df['month'].dtype != 'datetime64[ns]':\n",
    "        df['month'] = pd.to_datetime(df['month'])\n",
    "\n",
    "    first_3_years = df[df['month'] < '2012-01-01']\n",
    "\n",
    "    # Create a column for the year\n",
    "    first_3_years['year'] = first_3_years['month'].dt.year\n",
    "\n",
    "    # Calculate the 'Other types' column\n",
    "    first_3_years['Other_types'] = first_3_years.drop(columns=['month', 'year', 'P2PK', 'P2KH']).sum(axis=1)\n",
    "\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(10, 8), sharex=True)  \n",
    "\n",
    "    script_types = ['P2PK', 'P2KH', 'Other_types']\n",
    "    colors = ['tab:green', 'tab:orange', 'tab:purple']\n",
    "    \n",
    "    for i, script_type in enumerate(script_types):\n",
    "        sns.lineplot(ax=axes[i], data=first_3_years, x='month', y=script_type, color=colors[i], label=script_type)\n",
    "        axes[i].set_ylabel(f'{script_type} Count')\n",
    "        axes[i].set_title(f'Usage of {script_type} Over Time')\n",
    "        axes[i].legend(loc='upper left')\n",
    "\n",
    "    \n",
    "    axes[-1].set_xlabel('Month')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9, hspace=0.5)  \n",
    "    fig.suptitle('Script Types Usage in the First 3 Years of Bitcoin', y=0.97) \n",
    "    plt.show()   \n",
    "       \n",
    "def plot_annual_script_type_usage(df):\n",
    "    # Convert the 'month' column to datetime if it's not already\n",
    "    if df['month'].dtype != 'datetime64[ns]':\n",
    "         df['month'] = pd.to_datetime(df['month'])\n",
    "        \n",
    "    df = df[df['month'] < '2012-01-01']\n",
    "\n",
    "    # Extract the year from the 'month' column\n",
    "    df['year'] = df['month'].dt.year\n",
    "    \n",
    "    # Calculate the 'Other types' column\n",
    "    df['Other_types'] = df.drop(columns=['month', 'year', 'P2PK', 'P2KH']).sum(axis=1)\n",
    "    \n",
    "    # Group by year and sum the counts of each script type\n",
    "    annual_data = df.groupby('year')[['P2PK', 'P2KH', 'Other_types']].sum().reset_index()\n",
    "\n",
    "    # Melt the data for seaborn\n",
    "    annual_data_melted = pd.melt(annual_data, id_vars='year', var_name='scriptType', value_name='count')\n",
    "        \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.barplot(data=annual_data_melted, x='year', y='count', hue='scriptType', palette=['tab:green', 'tab:orange', 'tab:purple'])\n",
    "    plt.title('Annual Usage of Script Types')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title='Script Type')\n",
    "    plt.yscale('log')  #log scale to better visualize differences\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_blocks_mined_by_top_4_miners(blocks_mined_by_T4miners):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='addressId', y='blocks_mined', data=blocks_mined_by_T4miners)\n",
    "    plt.title('Total Blocks Mined by the top 4 miners')\n",
    "    plt.xlabel('Miner')\n",
    "    plt.ylabel('Total Blocks Mined')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_total_blocks_mined(global_blocks_mined):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='pool', y='blocks_mined', data=global_blocks_mined)\n",
    "    plt.title('Total Blocks Mined by Each Pool')\n",
    "    plt.xlabel('Mining Pool')\n",
    "    plt.ylabel('Total Blocks Mined')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_bi_monthly_blocks_mined(bi_monthly_blocks_mined):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.lineplot(x='bi_month', y='blocks_mined', hue='pool', data=bi_monthly_blocks_mined, marker='o')\n",
    "    plt.title('Blocks Mined Every 2 Months by Each Pool')\n",
    "    plt.xlabel('Time (Bi-Monthly Intervals)')\n",
    "    plt.ylabel('Blocks Mined')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_total_rewards(global_total_rewards):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='pool', y='total_rewards', data=global_total_rewards)\n",
    "    plt.title('Total Rewards Received by Each Pool')\n",
    "    plt.xlabel('Mining Pool')\n",
    "    plt.ylabel('Total Rewards (BTC)')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_bi_monthly_rewards(bi_monthly_total_rewards):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.lineplot(x='bi_month', y='total_rewards', hue='pool', data=bi_monthly_total_rewards, marker='o')\n",
    "    plt.title('Rewards Received Every 2 Months by Each Pool')\n",
    "    plt.xlabel('Time (Bi-Monthly Intervals)')\n",
    "    plt.ylabel('Total Rewards (BTC)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_Eligius_path(graph_DF):\n",
    "    # creazione di un grafo diretto\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # aggiungo nodi e archi al grafo\n",
    "    for _, row in graph_DF.iterrows():\n",
    "        txId = row['txId']\n",
    "        outputs = row['outputs']\n",
    "        for output in outputs:\n",
    "            G.add_edge(txId, output)\n",
    "\n",
    "    # disegno il grafo\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw(G, pos, with_labels=True, node_size=100, node_color='skyblue', font_size=3, font_weight='bold', arrowsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022b019a-58cb-4bdf-9526-a3da8818d91d",
   "metadata": {},
   "source": [
    "## Dataset analysis module \n",
    "\n",
    "**Modulo relativo all'analisi dei dati ed alle operazioni sui dataframes**\n",
    "\n",
    "In analizer.py ho scritto il codice relativo al calcolo della network congestion e delle fees per intervalli temporali di un mese sfruttando un meccanismo di threads ed operazioni vettoriali sul dataframe.\n",
    "Nel modulo vi sono anche le funzioni che calcolano la quantità di script per ogni tipo (sempre su intervalli temporali di un mese) e le funzioni relative al calcolo delle statische delle pools : total rewards e numero di blocchi minati da ogni pool sia globalmente che per ogni due mesi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c417425-e8fe-44c2-9d9c-0e87a26450e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time \n",
    "from utilities import SETTINGS, LOG_LEVELS\n",
    "\n",
    "pd.set_option(\"mode.copy_on_write\", True)\n",
    "SCRIPT_SIZE_MAP = {\n",
    "    1: 153,  # 'P2PK'\n",
    "    2: 180,  # 'P2KH'\n",
    "    3: 291   # 'P2SH'\n",
    "}\n",
    "\n",
    "SCRIPT_TYPE_MAP = {\n",
    "    0 : 'Unknown',\n",
    "    1: 'P2PK',   \n",
    "    2: 'P2KH',   \n",
    "    3: 'P2SH',   \n",
    "    4 : 'RETURN',\n",
    "    5 : 'EMPTY',\n",
    "    6 : 'P2WPKH',\n",
    "    7 : 'P2WSH',\n",
    "}\n",
    "\n",
    "\n",
    "MAX_THREAD_QUANTITY = SETTINGS['MAX_THREAD_QUANTITY']\n",
    "\n",
    "def processTransactions(inputsDF, outputsDF, transactionDF):\n",
    "    \"\"\"Process transactions using inputs, outputs and transaction dataframe:\n",
    "    •split transactions in months\n",
    "    •for each month calculate the network congestion and the fees \n",
    "    \n",
    "    @params : inputsDF : inputs dataframe \n",
    "    @params : outputsDF : outputs dataframe \n",
    "    @params : transactionDF : transactions dataframe \n",
    "    @return dataframe of months in which every month has network congestion and fees\n",
    "    \"\"\"\n",
    "    startT = time.time()\n",
    "    months = transactionDF.groupby(pd.Grouper(freq='ME'))\n",
    "    if LOG_LEVELS['processing'] or LOG_LEVELS['all infos'] or LOG_LEVELS['debug']:\n",
    "        print(f\"found {len(months)} months\")\n",
    "        print(\"------------------------\")\n",
    "    \n",
    "    def process_month(name, month):\n",
    "        \n",
    "        if not LOG_LEVELS['reduce spam'] and ( LOG_LEVELS['processing'] or LOG_LEVELS['all infos'] or LOG_LEVELS['debug']):\n",
    "            print(f\"processing month \\n{name.strftime('%Y-%m')}\")\n",
    "\n",
    "        month_data = {\n",
    "            'P2PK': 0,\n",
    "            'P2KH': 0,\n",
    "            'P2SH': 0,\n",
    "            'fees': 0,\n",
    "            'networkCongestion': 0,\n",
    "            'month': name.strftime('%Y-%m')\n",
    "        }\n",
    "\n",
    "        # Calcola la network congestion relativa al mese \n",
    "        month_data['networkCongestion'] = calculate_network_congestion(month, inputsDF, outputsDF)\n",
    "\n",
    "        # Calcola le fees relativa al mese \n",
    "        month_data['fees'] = month['fee'].sum()\n",
    "\n",
    "        # Calcola il numero di script type per ogni tipo\n",
    "        month_data.update(calculate_script_type_counts(month, outputsDF))\n",
    "\n",
    "        if not LOG_LEVELS['reduce spam'] and ( LOG_LEVELS['results'] or LOG_LEVELS['all infos'] or LOG_LEVELS['debug']):    \n",
    "            print(f\"got month data:\\n{month_data}\\n\")\n",
    "\n",
    "        return month_data\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(lambda x: process_month(*x), months))\n",
    "\n",
    "    result_df = pd.DataFrame(results)\n",
    "    result_df = result_df.loc[:, ~result_df.columns.str.match('None')]\n",
    "    \n",
    "    if LOG_LEVELS['time']:\n",
    "        print(f\"transactions processed in {time.time()-startT} seconds\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def calculate_network_congestion(month, inputsDF, outputsDF):\n",
    "    \"\"\"Calculate network congestion for single month:\n",
    "    \n",
    "    @params : month : single month dataframe \n",
    "    @params : inputsDF : inputs dataframe \n",
    "    @params : outputsDF : outputs dataframe \n",
    "\n",
    "    @return network congestion related to the month passed by argument\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calcolo del numero di input per ogni transazione nel mese\n",
    "    month_inputs = inputsDF[inputsDF['txId'].isin(month['txId'])]\n",
    "    n_inputs_per_tx = month_inputs.groupby('txId').size()\n",
    "\n",
    "    # Calcolo del numero di output per ogni transazione nel mese\n",
    "    month_outputs = outputsDF[outputsDF['txId'].isin(month['txId'])]\n",
    "    n_outputs_per_tx = month_outputs.groupby('txId').size()\n",
    "\n",
    "    # Recupera il tipo di script per ogni transazione nel mese\n",
    "    # (prende il primo poiché i tipi di script relativi agli outputs della stessa transazione dovrebbero esser tutti uguali)\n",
    "    script_types_per_tx = month_outputs.groupby('txId')['scriptType'].first()\n",
    "\n",
    "    # Calcolo della dimensione di ogni transazione\n",
    "    tx_sizes = 40 * n_inputs_per_tx + 9 * n_outputs_per_tx + script_types_per_tx.map(SCRIPT_SIZE_MAP).fillna(153)\n",
    "\n",
    "    # Somma dele dimensioni delle transazioni per ottenere la network congestion del mese\n",
    "    return tx_sizes.sum()\n",
    "\n",
    "def calculate_script_type_counts(month, outputsDF):\n",
    "    \"\"\"Calculate script type quantity for each script type in a month:\n",
    "    \n",
    "    @params : month : (single) month dataframe \n",
    "    @params : outputsDF : outputs dataframe \n",
    "\n",
    "    @return dictionary with script type quantity for each script type\n",
    "    \"\"\"\n",
    "    \n",
    "    # Numero di outputs per ogni script type\n",
    "    script_type_counts = outputsDF[outputsDF['txId'].isin(month['txId'])].groupby('scriptType').size()\n",
    "    # Mappa i tipi di script con i relativi nomi\n",
    "    script_type_counts.index = script_type_counts.index.map(SCRIPT_TYPE_MAP.get)\n",
    "    # Costruisci il dizionario tipo di script <--> quantità\n",
    "    script_type_dict = {f'{script_type}': count for script_type, count in script_type_counts.items()}\n",
    "    return script_type_dict\n",
    "\n",
    "def calculate_pool_statistics(df):\n",
    "    \"\"\"Calculate the number of minted blocks and the total rewards for each pool of a dataframe. \n",
    "    \n",
    "    @params : df : dataframe (of Coinbase transactions associated to a pool) \n",
    "\n",
    "    @return dataframe with minted blocks and dataframe with total rewards for each pool\n",
    "    \"\"\"\n",
    "        \n",
    "    grouped = df.groupby('pool')\n",
    "    \n",
    "    # Ottiene il numero di blocchi minati da ciascuna pool\n",
    "    blocks_mined = grouped['blockId'].nunique().reset_index(name='blocks_mined')\n",
    "    \n",
    "    # Ottiene le reward totali ricevute da ciascuna pool\n",
    "    total_rewards = grouped['amount'].sum().reset_index(name='total_rewards')\n",
    "    \n",
    "    return blocks_mined, total_rewards\n",
    "\n",
    "def calculate_bi_monthly_statistics(df):\n",
    "    \"\"\"Calculate the number of minted blocks and the total rewards \n",
    "    for every time period of 2 months for each pool of a dataframe. \n",
    "    \n",
    "    @params : df : dataframe (of Coinbase transactions associated to a pool) \n",
    "\n",
    "    @return dataframe with minted blocks for every two months and dataframe with total rewards for every two months for each pool\n",
    "    \"\"\"\n",
    "    \n",
    "    # Aggiungo una colonna al dataframe per ogni due mesi \n",
    "    df['bi_month'] = df['timestamp'].apply(lambda x: f\"{x.year}-{(x.month-1)//2*2+1:02d}\")\n",
    "\n",
    "    # Raggruppo il dataframe per pool e periodo di due mesi\n",
    "    grouped_bi_monthly = df.groupby(['pool', 'bi_month'])\n",
    "    \n",
    "    # Ottengo il numero di blocchi minati per intervallo di due mesi\n",
    "    blocks_mined_bi_monthly = grouped_bi_monthly['blockId'].nunique().reset_index(name='blocks_mined')\n",
    "    \n",
    "    # Ottengo le reward totali per intervallo di due mesi\n",
    "    total_rewards_bi_monthly = grouped_bi_monthly['amount'].sum().reset_index(name='total_rewards')\n",
    "    \n",
    "    return blocks_mined_bi_monthly, total_rewards_bi_monthly\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b44a96f-6dc7-42c7-b11d-7213ef968495",
   "metadata": {},
   "source": [
    "## Scraping module \n",
    "\n",
    "**Modulo relativo allo scraping dei dati**\n",
    "\n",
    "In questo modulo ho inserito tutte le funzioni relative allo scraping dei dati su WalletExplorer.\n",
    "\n",
    "Nel dettaglio, i dati relativi alle pools sono ottenuti in due modi, sia sfruttando Selenium nella funzione get_W_addresses_Selenium sia tramite BeautifulSoup nella funzione getWalletAddresses.\n",
    "\n",
    "All'interno del codice ho definito un dizionario per mantenere le proxies ottenute da sslproxies.org che sfrutta 'last search time' per evitare too many requests su sslproxies.org.\n",
    "Il codice comprende inoltre alcune altre utili funzioni volte a migliorare quanto più possibile le requests, in particolare getRequestUtils fornisce headers e proxies da sfruttare per la request costruendoli dinamicamente e randomicamente (è inoltre possibile passarle True come argomento per scegliere di utilizzare un referrer specifico tra quelli relativi a WalletExplorer e di forzare la generazione di nuove proxies ottenute da generate_proxies -a cui verrà passato True come parametro da getRequestUtils - )\n",
    "\n",
    "Per ottenere i dati da WalletExplorer ho deciso inizialmente di lavorare con Selenium poiché ritenevo ardua la sfida di non avere blocchi nel fare oltre 70 requests consecutive per ottenere gli address della pool BTCGuild.Per tal motivo ho creato get_W_addresses_Selenium che ottiene il numero di pagine totali per la pool considerata e, sfruttando i threads, ottiene gli indirizzi di ogni pagina, distribuendo il lavoro.\n",
    "Con questo approccio ero soddisfatto per le pools di Eligius e BitMinter, ma non per BTCGuild, per cui il codice impiegava oltre 5 minuti ad ottenere gli addresses.\n",
    "Quindi ho deciso di implementare le funzioni per supportare le requests e ho migliorato getWalletAddresses, andando inoltre a creare getWalletAddress_multiplePages (corrispettiva di get_W_addresses_Selenium).In essa inizialmente avevo introdotto un meccanismo di threads sempre al fine di migliorare il più possibile le prestazioni, ma la frequenza delle requests era troppo elevata e, nonostante gli accorgimenti, ottenevo blocchi (429 e too many requests).\n",
    "Ho quindi optato per la rimozione dei threads in getWalletAddress_multiplePages e, con ulteriori migliorie relative al time.sleep in getWalletAddresses, son riuscito ad ottenere una media di poco più di due minuti senza ricevere alcuna limitazione nelle requests.\n",
    "\n",
    "\n",
    "Infine, per quanto riguarda la taint analysis di Eligius, ho associato un nodo del grafo ad una transazione, dove :\n",
    "- l'id della transazione identifica anche il nodo\n",
    "- vi è un arco tra due transazioni se l'output di una è input dell'altra\n",
    "\n",
    "La funzione che ottiene i nodi :\n",
    "- Parte dalla transazione della Coinbase Eligius e procede per k step.\n",
    "- Allo step 1 ottiene gli input e gli output della prima transazione e salva gli output per lo step successivo.\n",
    "- Allo step i-esimo (i >= 2) considera gli ID delle transazioni degli output dello step precedente (i-1) come nodi e per ognuno di essi trova input e output. Inoltre, salva come nuovi output per l'iterazione successiva (i+1) tutti gli output dell'iterazione corrente.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb924d81-c562-4382-8cad-ceaf82b2f5c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "greenlet.greenlet size changed, may indicate binary incompatibility. Expected 152 from C header, got 40 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor, as_completed\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfake_useragent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UserAgent\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Request, urlopen\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m \n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/fake_useragent/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute_import, unicode_literals\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfake_useragent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfake\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FakeUserAgent, UserAgent  \u001b[38;5;66;03m# noqa # isort:skip\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfake_useragent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FakeUserAgentError, UserAgentError  \u001b[38;5;66;03m# noqa # isort:skip\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfake_useragent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msettings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m VERSION  \u001b[38;5;66;03m# noqa # isort:skip\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/fake_useragent/fake.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfake_useragent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FakeUserAgentError\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfake_useragent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlog\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfake_useragent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load, load_cached, str_types, update\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFakeUserAgent\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     16\u001b[0m         cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m         safe_attrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(),\n\u001b[1;32m     22\u001b[0m     ):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/fake_useragent/utils.py:31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msocket\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgevent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msocket\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m gevent\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39msocket:\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgevent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sleep\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gevent/__init__.py:72\u001b[0m\n\u001b[1;32m     69\u001b[0m setswitchinterval \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39msetswitchinterval\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgevent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgevent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_hub_local\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_hub\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgevent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_hub_primitives\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m iwait_on_objects \u001b[38;5;28;01mas\u001b[39;00m iwait\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgevent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_hub_primitives\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wait_on_objects \u001b[38;5;28;01mas\u001b[39;00m wait\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gevent/_hub_local.py:150\u001b[0m\n\u001b[1;32m    147\u001b[0m     _threadlocal\u001b[38;5;241m.\u001b[39mloop \u001b[38;5;241m=\u001b[39m loop\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgevent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m import_c_accel\n\u001b[0;32m--> 150\u001b[0m \u001b[43mimport_c_accel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgevent.__hub_local\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gevent/_util.py:148\u001b[0m, in \u001b[0;36mimport_c_accel\u001b[0;34m(globs, cname)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# Python 3.7 likes to produce\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# \"ImportWarning: can't resolve\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Cython, but it doesn't seem to have any consequences, it's\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# just annoying to see and can mess up our unittests.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;167;01mImportWarning\u001b[39;00m)\n\u001b[0;32m--> 148\u001b[0m     mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# By adopting the entire __dict__, we get a more accurate\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# __file__ and module repr, plus we don't leak any imported\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# things we no longer need.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m globs\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32msrc/gevent/_hub_local.py:1\u001b[0m, in \u001b[0;36minit gevent._gevent_c_hub_local\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 152 from C header, got 40 from PyObject"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from fake_useragent import UserAgent\n",
    "from urllib.request import Request, urlopen\n",
    "import random \n",
    "\n",
    "# Aggiunge la directory contenente utilities.py al percorso di ricerca dei moduli\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "from utilities import LOG_LEVELS, SETTINGS\n",
    "\n",
    "HEADLESS_MODE = SETTINGS['SELENIUM_HEADLESS_MODE']\n",
    "BASE_LINK = \"https://www.walletexplorer.com\"\n",
    "MAX_THREAD_QUANTITY = SETTINGS['MAX_THREAD_QUANTITY']\n",
    "\n",
    "ELIGIUS_COINBASE_TX = 'c82c10925cc3890f1299'\n",
    "\n",
    "sslproxies_infos = {\n",
    "    'last search time': None,\n",
    "    'sslproxies' : []\n",
    "}\n",
    "\n",
    "referrer_list = [\n",
    "    \"https://www.google.com\",\n",
    "    \"https://www.bing.com\",\n",
    "    \"https://search.yahoo.com\",\n",
    "    \"https://www.duckduckgo.com\",\n",
    "    \"https://www.baidu.com\",\n",
    "    \"https://www.yandex.ru\",\n",
    "    \"https://www.aol.com\",\n",
    "    \"https://www.wikipedia.org\",\n",
    "    \"https://www.facebook.com\",\n",
    "    \"https://www.twitter.com\",\n",
    "    \"https://www.linkedin.com\",\n",
    "    \"https://www.instagram.com\",\n",
    "    \"https://www.reddit.com\",\n",
    "    \"https://www.pinterest.com\",\n",
    "    \"https://www.quora.com\",\n",
    "    \"https://www.medium.com\",\n",
    "    \"https://news.ycombinator.com\",\n",
    "    \"https://www.bbc.com\",\n",
    "    \"https://www.cnn.com\",\n",
    "    \"https://www.nytimes.com\",\n",
    "    \"https://www.theguardian.com\",\n",
    "    \"https://www.forbes.com\",\n",
    "    \"https://www.bloomberg.com\"\n",
    "]\n",
    "\n",
    "specific_referrer_list = [ # referrer form walletexplorer pages \n",
    "    'https://www.walletexplorer.com/wallet/7644ed877fa28a88?from_address=1G43MvhzCqRz1ctsQUmgU4LgLuSVdfU557',\n",
    "    'https://www.walletexplorer.com',\n",
    "    'https://www.walletexplorer.com/?q=',\n",
    "    'https://www.walletexplorer.com/wallet/DeepBit.net',\n",
    "    'https://www.walletexplorer.com/wallet/DeepBit.net/addresses',\n",
    "    'https://www.walletexplorer.com/info',\n",
    "    'https://www.walletexplorer.com/privacy',\n",
    "    'https://www.walletexplorer.com/wallet/BTCCPool',\n",
    "]\n",
    "\n",
    "class RequestError(Exception):\n",
    "    def __init__(self, message, erroCode):\n",
    "        super().__init__(message)\n",
    "        self.erroCode = erroCode\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.erroCode}: {self.args[0]}\"\n",
    "\n",
    "def generate_proxies(forceNew = False): \n",
    "    \"\"\"generate and return (free) proxies by sslproxies.org if \n",
    "    the last request made to sslproxies is at least 3 seconds ago, \n",
    "    otherwise it returns the last generated proxies\n",
    "    \n",
    "    @param forceNew : if True the function will request new proxies to sslproxies.org\n",
    "    @return : proxies list with ip & port for every proxy\n",
    "    \"\"\"\n",
    "    proxies = []\n",
    "    lastReqTime = sslproxies_infos['last search time']\n",
    "    \n",
    "    if forceNew or (lastReqTime is None or time.time() - lastReqTime > 3):\n",
    "        user_agent = getRandomUserAgent()\n",
    "        proxies_req = Request('https://www.sslproxies.org/')\n",
    "        proxies_req.add_header('User-Agent', user_agent)\n",
    "        proxies_doc = urlopen(proxies_req).read().decode('utf8')\n",
    "        soup = bs(proxies_doc, 'html.parser')\n",
    "        proxies_table = soup.find('table', class_='table table-striped table-bordered')        \n",
    "        \n",
    "        for row in proxies_table.tbody.find_all('tr'):\n",
    "            td = row.find_all('td')\n",
    "            proxies.append({\n",
    "            'ip':   td[0].string,\n",
    "            'port': td[1].string})\n",
    "\n",
    "        sslproxies_infos['sslproxies'] = proxies\n",
    "        sslproxies_infos['last search time'] = time.time()\n",
    "    else:\n",
    "        proxies = sslproxies_infos['sslproxies']\n",
    "  \n",
    "    return proxies\n",
    "\n",
    "def getRandomUserAgent():\n",
    "    \"\"\"generate and return a random user agent\n",
    "    @no params\n",
    "    @return : random user agent string\n",
    "    \"\"\"\n",
    "    ua = UserAgent()     \n",
    "    return ua.random\n",
    "\n",
    "def getRequestUtils(specific = False):\n",
    "    \"\"\"Returns utils for a request (headers, proxies)\n",
    "    @param specific : if True the function will use specific proxies and referrer \n",
    "    @return : headers, proxies to use for a request / request session\n",
    "    \"\"\"\n",
    "    \n",
    "    # Liste di opzioni dinamiche per gli headers\n",
    "    accept_language_options = [\n",
    "        \"it-IT,it;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "        \"en-US,en;q=0.9,it-IT;q=0.8,it;q=0.7\",\n",
    "        \"fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7\"\n",
    "    ]\n",
    "\n",
    "    sec_ch_ua_options = [\n",
    "        \"\\\"Chromium\\\";v=\\\"122\\\", \\\"Not(A:Brand\\\";v=\\\"24\\\", \\\"Google Chrome\\\";v=\\\"122\\\"\",\n",
    "        \"\\\"Opera\\\";v=\\\"80\\\", \\\"Chromium\\\";v=\\\"94\\\", \\\"Not(A:Brand\\\";v=\\\"24\\\"\",\n",
    "        \"\\\"Microsoft Edge\\\";v=\\\"90\\\", \\\"Chromium\\\";v=\\\"90\\\", \\\"Not(A:Brand\\\";v=\\\"24\\\"\"\n",
    "    ]\n",
    "\n",
    "    sec_ch_ua_platform_options = [\n",
    "        \"\\\"macOS\\\"\",\n",
    "        \"\\\"Windows\\\"\",\n",
    "        \"\\\"Linux\\\"\"\n",
    "    ]\n",
    "\n",
    "    user_A = getRandomUserAgent()\n",
    "    if specific : \n",
    "        referrer = random.choice(specific_referrer_list)        \n",
    "    else:\n",
    "        referrer = random.choice(referrer_list)\n",
    "    \n",
    "    proxies = generate_proxies(specific)\n",
    "    \n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': user_A,\n",
    "        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "        \"cache-control\": \"max-age=0\",\n",
    "        \"accept-language\": random.choice(accept_language_options),\n",
    "        \"sec-ch-ua\": random.choice(sec_ch_ua_options),\n",
    "        \"sec-ch-ua-platform\": random.choice(sec_ch_ua_platform_options),\n",
    "        \"sec-fetch-dest\": \"document\",\n",
    "        \"sec-fetch-mode\": \"navigate\",\n",
    "        \"sec-fetch-site\": \"same-origin\",\n",
    "        \"sec-fetch-user\": \"?1\",\n",
    "        \"upgrade-insecure-requests\": \"1\",\n",
    "        'referer': referrer,\n",
    "        \"referrerPolicy\": \"strict-origin-when-cross-origin\",\n",
    "        \"credentials\": \"omit\"\n",
    "\n",
    "    }\n",
    "    \n",
    "    return headers, proxies\n",
    "              \n",
    "def getWalletAddresses(url):  \n",
    "    \"\"\"Get wallet addresses associated to a pool by the WalletExplorer's url. \n",
    "    \n",
    "    @params : url : url of the WalletExplorer address page of a pool.\n",
    "\n",
    "    @return : list of addresses associated to the pool\n",
    "    \"\"\"\n",
    "    \n",
    "    startT = time.time()\n",
    "    \n",
    "    if LOG_LEVELS['time']:\n",
    "        print(\"get wallet addresses started\")\n",
    "        \n",
    "    if LOG_LEVELS['all infos'] or LOG_LEVELS['debug']:\n",
    "        print(f\"going to get wallet addresses with url {url}\") \n",
    "         \n",
    "    headers, proxies = getRequestUtils()\n",
    "    \n",
    "    addresses = [] \n",
    "    \n",
    "  \n",
    "    time.sleep(0.75)\n",
    "    timeBeforeRequest = time.time()\n",
    "    response = requests.get(url,headers=headers, proxies=random.choice(proxies)) \n",
    "    if LOG_LEVELS['time']:\n",
    "        print(f\"request time = {time.time()-timeBeforeRequest} seconds\")\n",
    "    attempt = 0\n",
    "    while attempt < 20 and (response.text.startswith(\"Too\") or response.status_code < 200 or response.status_code > 299):\n",
    "        attempt+= 1\n",
    "        if LOG_LEVELS['debug']:\n",
    "            print(f\"Unsuccessful response in attempt {attempt}/15 - response status code = {response.status_code}\")\n",
    "        \n",
    "        new_headers, new_proxies = getRequestUtils(specific=True)\n",
    "        chosenProxy = random.choice(new_proxies)\n",
    "        time.sleep(5)\n",
    "        response = requests.get(url, proxies=chosenProxy, headers=new_headers)        \n",
    "        \n",
    "        \n",
    "        if attempt == 20:\n",
    "            if LOG_LEVELS['debug']:\n",
    "                print(f'unsuccessful response:\\n{response}\\nTxt:\\n{response.text}')\n",
    "            raise RequestError(f'Error in get wallet address request\\nresponse status = {response.status_code}\\nsession headers = {new_headers}\\nchosen proxy = {chosenProxy}', response.status_code)\n",
    "                \n",
    "\n",
    "\n",
    "    html_content = response.text\n",
    "    soup = bs(html_content,'html.parser')\n",
    "    \n",
    "    table = soup.find('table') \n",
    "    if table:\n",
    "        trs = table.findAll('tr')\n",
    "        \n",
    "        if not LOG_LEVELS['reduce spam'] and ( LOG_LEVELS['all infos'] or LOG_LEVELS['debug']):\n",
    "            print(f\"going to process {len(trs)} table rows\")  \n",
    "    \n",
    "        i = 0\n",
    "        for tr in trs:\n",
    "            if not LOG_LEVELS['reduce spam'] and (LOG_LEVELS['all infos'] or LOG_LEVELS['debug'] or LOG_LEVELS['processing']):\n",
    "                print(f\"processing row {i+1}/{len(trs)}\") \n",
    "            i+=1\n",
    "            td = tr.find('td')  # Find the first td element\n",
    "            \n",
    "            \n",
    "            if td and td.a:  # Check if td and td.a exist\n",
    "                href = td.a['href']\n",
    "                address = href.split(\"/\")[-1]\n",
    "                addresses.append(address)  # Use append instead of push\n",
    "                if not LOG_LEVELS['reduce spam'] and (LOG_LEVELS['all infos'] or LOG_LEVELS['debug'] or LOG_LEVELS['processing']):\n",
    "                    print(f\"-> found  wallet : {address}\") \n",
    "      \n",
    "    if LOG_LEVELS['all infos'] or LOG_LEVELS['debug'] or LOG_LEVELS['results']:\n",
    "        print(f\"Found {len(addresses)} wallets\") \n",
    "    \n",
    "    if LOG_LEVELS['time']:\n",
    "        print(f\"get wallet addresses ended in {time.time()-startT} seconds\")\n",
    "        \n",
    "    return addresses\n",
    "\n",
    "def getWalletAddress_multiplePages(url):\n",
    "    \"\"\"Finds and returns wallet addresses for a mininng pool also if it has addresses in more than one page\n",
    "    @param url : url of the first page of addresses of a pool\n",
    "    @return : all wallet addresses found for the pool    \n",
    "    \"\"\"\n",
    "    \n",
    "      \n",
    "    startT = time.time()\n",
    "    if LOG_LEVELS['time']:\n",
    "        print(\"get wallet addresses (with Selenium) started\")\n",
    "    driver = setup_selenium_driver()\n",
    "    num_pages = get_number_of_pages(driver, url)\n",
    "    driver.quit()\n",
    "    \n",
    "    addresses = []\n",
    "    \n",
    "    for page_number in range(num_pages+1):\n",
    "        if LOG_LEVELS['debug'] or LOG_LEVELS['all infos'] or LOG_LEVELS['processing']:\n",
    "            print(f\"processing page {page_number}\")\n",
    "            \n",
    "\n",
    "        if page_number != 1:\n",
    "            page_url = f\"{url}?page={page_number}\"\n",
    "        else:\n",
    "            page_url = url\n",
    "                    \n",
    "        curent_addresses = getWalletAddresses(page_url)\n",
    "        addresses += curent_addresses\n",
    "                \n",
    "   \n",
    "            \n",
    "    if LOG_LEVELS['debug'] or LOG_LEVELS['all infos'] or LOG_LEVELS['results']:\n",
    "        print(f\"Processed {num_pages} pages and found {len(addresses)} addresses\")\n",
    "    \n",
    "    if LOG_LEVELS['time']:\n",
    "        print(f\"get wallet addresses (with multiple pages) ended in {time.time()-startT} seconds\")\n",
    "    return addresses\n",
    "  \n",
    "def setup_selenium_driver():\n",
    "    \"\"\"Setup the Selenium driver and return it.\n",
    "    \n",
    "    @params : no params\n",
    "\n",
    "    @return : Selenium web driver (webdriver.Chrome)\n",
    "    \"\"\"\n",
    "    options = Options()\n",
    "    if HEADLESS_MODE:\n",
    "        options.add_argument('--headless=new')\n",
    "        if LOG_LEVELS['debug'] or LOG_LEVELS['all infos']:\n",
    "            print(\"Using Selenium in headless mode...\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "def get_number_of_pages(driver, url):\n",
    "    \"\"\"Finds and returns the number of pages associated to a pool.\n",
    "    \n",
    "    @params : driver : Selenium web driver\n",
    "    @params : url : url of the first addresses page of a pool\n",
    "\n",
    "    @return : quantity of addresses pages of the pool\n",
    "    \"\"\"\n",
    "    \n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    paging_info = driver.find_elements(By.XPATH, '//*[@id=\"main\"]/div[1]')[0]\n",
    "    page_text = paging_info.text\n",
    "    \n",
    "    match = re.search(r'Page 1 / (\\d+)', page_text)\n",
    "    if match:\n",
    "        num_pages = int(match.group(1))\n",
    "        if LOG_LEVELS['debug'] or LOG_LEVELS['all infos'] or LOG_LEVELS['processing']:\n",
    "            print(f\"Found {num_pages} pages to process\")\n",
    "    else:\n",
    "        num_pages = 1  # Default to 1 if the number of pages cannot be found\n",
    "    \n",
    "    return num_pages\n",
    "\n",
    "def get_addresses_from_page(driver, url, page_number):\n",
    "    \"\"\"Finds and returns the addresses associated to a pool.\n",
    "    \n",
    "    @params : driver : Selenium web driver\n",
    "    @params : url : url of the page of the page from which to obtain the addresses\n",
    "\n",
    "    @return : addresses found in the page\n",
    "    \"\"\"\n",
    "    \n",
    "    addresses = []\n",
    "    \n",
    "    try:\n",
    "        if page_number != 1:\n",
    "            new_url = f\"{url}?page={page_number}\"\n",
    "            driver.get(new_url)\n",
    "            time.sleep(0.2)\n",
    "        else:\n",
    "            driver.get(url)\n",
    "        \n",
    "        if LOG_LEVELS['debug'] or LOG_LEVELS['all infos'] or LOG_LEVELS['processing']:\n",
    "            print(f\"processing page {page_number}\")\n",
    "            \n",
    "        table = driver.find_element(By.TAG_NAME, 'table')\n",
    "        trs = table.find_elements(By.TAG_NAME, 'tr')\n",
    "        \n",
    "        for i, tr in enumerate(trs, start=1):\n",
    "            td_list = tr.find_elements(By.TAG_NAME, 'td')\n",
    "            if len(td_list) > 0:\n",
    "                td = td_list[0]\n",
    "                try:\n",
    "                    a_tag = td.find_element(By.TAG_NAME, 'a')\n",
    "                    if a_tag:\n",
    "                        href = a_tag.get_attribute('href')\n",
    "                        address = href.split(\"/\")[-1]\n",
    "                        addresses.append(address)\n",
    "                        if not LOG_LEVELS['reduce spam'] and (LOG_LEVELS['debug'] or LOG_LEVELS['all infos'] or LOG_LEVELS['processing']):\n",
    "                            print(f\"-> found address {address}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    if LOG_LEVELS['debug'] or LOG_LEVELS['all infos'] or LOG_LEVELS['processing']:\n",
    "                        print(f\"address {i} is not a standard address -> skipped\")\n",
    "                        \n",
    "                    if LOG_LEVELS['debug']:\n",
    "                        print(f\"error:\\n{e}\")\n",
    "\n",
    "                    continue\n",
    "    except Exception as e:\n",
    "        if LOG_LEVELS['debug']:\n",
    "            print(f\"error in get address from page:\\n{e}\")\n",
    "    \n",
    "    finally:\n",
    "        return addresses\n",
    "\n",
    "def get_W_addresses_Selenium(url):\n",
    "    \"\"\"Finds and returns all the wallet addresses associated to a pool.\n",
    "    \n",
    "    @params : url : url of the first page of addresses of a pool\n",
    "\n",
    "    @return : all wallet addresses found for the pool\n",
    "    \"\"\"\n",
    "    \n",
    "    startT = time.time()\n",
    "    if LOG_LEVELS['time']:\n",
    "        print(\"get wallet addresses (with Selenium) started\")\n",
    "    driver = setup_selenium_driver()\n",
    "    num_pages = get_number_of_pages(driver, url)\n",
    "    driver.quit()\n",
    "    \n",
    "    # Imposta MAX_THREAD_QUANTITY a num_pages se è maggiore\n",
    "    max_t_quantity = min(MAX_THREAD_QUANTITY, num_pages)\n",
    "    addresses = []\n",
    "    \n",
    "    def process_pages(start_page, end_page): #processa le pagine da start_page a end_page\n",
    "        local_driver = setup_selenium_driver()\n",
    "        local_addresses = []\n",
    "        for page in range(start_page, end_page + 1):\n",
    "            curent_addresses = get_addresses_from_page(local_driver, url, page)\n",
    "            local_addresses += curent_addresses\n",
    "        \n",
    "        local_driver.quit()\n",
    "        return local_addresses\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_t_quantity) as executor:\n",
    "        future_to_page = {executor.submit(process_pages, i, min(i + num_pages // max_t_quantity, num_pages)): i for i in range(1, num_pages + 1, num_pages // max_t_quantity)}\n",
    "        for future in as_completed(future_to_page):\n",
    "            results = future.result()\n",
    "            addresses += results\n",
    "            \n",
    "    if LOG_LEVELS['debug'] or LOG_LEVELS['all infos'] or LOG_LEVELS['results']:\n",
    "        print(f\"Processed {num_pages} pages and found {len(addresses)} addresses\")\n",
    "    \n",
    "    if LOG_LEVELS['time']:\n",
    "        print(f\"get wallet addresses (with Selenium) ended in {time.time()-startT} seconds\")\n",
    "    return addresses\n",
    "       \n",
    "def getPools(): \n",
    "    \"\"\"Finds and returns all the addresses associated with each of the 4 mining pools considered\n",
    "    \n",
    "    @params : no params\n",
    "\n",
    "    @return : dataframe of transactions associated to a pool\n",
    "    \"\"\"\n",
    "\n",
    "    startT = time.time()\n",
    "    if LOG_LEVELS['time']:\n",
    "        print(\"Get pools started\")\n",
    "            \n",
    "    eligiusAddressesLink = f\"{BASE_LINK}/wallet/Eligius.st/addresses\"\n",
    "    deepBitAddressesLink = BASE_LINK+'/wallet/DeepBit.net'+'/addresses'    \n",
    "    bitMinterAddressesLink = BASE_LINK+'/wallet/BitMinter.com'+'/addresses'    \n",
    "    BTCGuildAddressesLink = BASE_LINK + '/wallet/BTCGuild.com'+ '/addresses'\n",
    "    \n",
    "    \n",
    "    BTCGuild_WalletAddresses = getWalletAddress_multiplePages(BTCGuildAddressesLink)\n",
    "    \n",
    "    eligius_WalletAddresses = get_W_addresses_Selenium(eligiusAddressesLink)\n",
    "    \n",
    "    deepBit_WalletAddresses = getWalletAddresses(deepBitAddressesLink)\n",
    "    \n",
    "    bitMinter_WalletAddresses = get_W_addresses_Selenium(bitMinterAddressesLink)\n",
    "    \n",
    "    \n",
    "    \n",
    "    pools = {\n",
    "        'Eligius' : eligius_WalletAddresses,\n",
    "        'DeepBit' : deepBit_WalletAddresses,\n",
    "        'BitMinter' : bitMinter_WalletAddresses,\n",
    "        'BTCGuild' : BTCGuild_WalletAddresses,\n",
    "    }\n",
    "    \n",
    "    # Creo una lista di tuple (address, pool) per poi poterla convertire in un dataframe \n",
    "    pool_data = [(address, pool) for pool, addresses in pools.items() for address in addresses]\n",
    "\n",
    "    # Converto la lista di tuple in un DataFrame con colonne txHash e pool\n",
    "    df_pools = pd.DataFrame(pool_data, columns=['txHash', 'pool'])\n",
    "    \n",
    "    if LOG_LEVELS['time']:\n",
    "        print(f\"get pools ended in {time.time()-startT} seconds \")\n",
    "    \n",
    "    return df_pools\n",
    "    \n",
    "\n",
    "def getTxAsNode(txId):\n",
    "    \"\"\"Searchs the single transaction by it's hash on WalletExplorer, \n",
    "    finds transaction's inputs and outputs and return the transaction as a node\n",
    "    \n",
    "    @params : txId : transaction hash \n",
    "\n",
    "    @return : the node related to the transaction as a dictionary with txId, inputs and outputs keys\n",
    "    \"\"\"\n",
    "\n",
    "    tx_node = {\n",
    "        'txId': txId,\n",
    "        'inputs': [],\n",
    "        'outputs': [],        \n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        driver = setup_selenium_driver()\n",
    "        if(txId == ELIGIUS_COINBASE_TX):\n",
    "            driver.get(BASE_LINK)\n",
    "            time.sleep(0.5)        \n",
    "            \n",
    "            inputSpace = driver.find_element(By.XPATH, '/html/body/div[2]/form/p/label/input')\n",
    "            submitButton = driver.find_element(By.XPATH, '/html/body/div[2]/form/p/input')\n",
    "            \n",
    "            inputSpace.send_keys(txId)    \n",
    "            submitButton.click()\n",
    "        else:\n",
    "            link = f'{BASE_LINK}/txid/{txId}'\n",
    "            \n",
    "            driver.get(link)\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        \n",
    "        infoTable = driver.find_element(By.CLASS_NAME, 'info')        \n",
    "        infoTableBody = infoTable.find_element(By.TAG_NAME,'tbody')\n",
    "        infoTableFirstTr = infoTableBody.find_elements(By.TAG_NAME,'tr')[0]\n",
    "        tx_node['txId'] = infoTableFirstTr.find_element(By.TAG_NAME,'td').text\n",
    "        \n",
    "        txTable = driver.find_element(By.XPATH, '/html/body/div[2]/table[2]')\n",
    "        txTableBody = txTable.find_element(By.TAG_NAME,'tbody')\n",
    "        txTableSecondTr = txTableBody.find_element(By.XPATH,'/html/body/div[2]/table[2]/tbody/tr[2]')\n",
    "        \n",
    "        raw_inputs = txTableSecondTr.find_element(By.XPATH, '/html/body/div[2]/table[2]/tbody/tr[2]/td[1]')\n",
    "        raw_outputs = txTableSecondTr.find_element(By.XPATH, '/html/body/div[2]/table[2]/tbody/tr[2]/td[2]')    \n",
    "        raw_inputs_trs = raw_inputs.find_element(By.TAG_NAME,'tbody').find_elements(By.TAG_NAME,'tr')\n",
    "        raw_outputs_trs = raw_outputs.find_element(By.TAG_NAME,'tbody').find_elements(By.TAG_NAME,'tr')\n",
    "        \n",
    "        inputs = []\n",
    "        for tr in raw_inputs_trs:\n",
    "            if txId == ELIGIUS_COINBASE_TX:\n",
    "                firstTd = tr.find_elements(By.TAG_NAME,'td')[0]\n",
    "                inputTxt = firstTd.text\n",
    "                inputs.append(inputTxt)\n",
    "                \n",
    "            else:\n",
    "                firstTd = tr.find_element(By.CLASS_NAME,'small')\n",
    "                a_tag = firstTd.find_element(By.TAG_NAME, 'a')\n",
    "                href = a_tag.get_attribute('href')        \n",
    "                \n",
    "                if 'txid' in href:\n",
    "                    txid = href.split('/txid/')[1]\n",
    "                    inputs.append(txid)\n",
    "                else:\n",
    "                    inputTxt = a_tag.text\n",
    "                    inputs.append(inputTxt)\n",
    "            \n",
    "        tx_node['inputs'] = inputs\n",
    "        \n",
    "        outputs = []\n",
    "        for tr in raw_outputs_trs:\n",
    "            firstTd = tr.find_element(By.CLASS_NAME,'small')\n",
    "            try:\n",
    "                a_tag = firstTd.find_element(By.TAG_NAME, 'a')\n",
    "                href = a_tag.get_attribute('href')        \n",
    "                \n",
    "                if 'txid' in href:\n",
    "                    txid = href.split('/txid/')[1]\n",
    "                    outputs.append(txid)\n",
    "                else:\n",
    "                    outputTxt = a_tag.text\n",
    "                    outputs.append(outputTxt)\n",
    "\n",
    "            except Exception as e :\n",
    "                if 'unspent' in firstTd.text and (LOG_LEVELS['debug'] or LOG_LEVELS['all infos']):\n",
    "                    print(\"unspent tx -> skip it\")                \n",
    "                elif LOG_LEVELS['debug']:\n",
    "                    print(f\"error = {e}\")\n",
    "        \n",
    "        tx_node['outputs'] = outputs\n",
    "        \n",
    "        if not LOG_LEVELS['reduce spam'] and (LOG_LEVELS['debug'] or LOG_LEVELS['all infos'] or LOG_LEVELS['processing']):\n",
    "            print(f\"-> found node:\\n {tx_node}\")\n",
    "    except Exception as e:\n",
    "        if LOG_LEVELS['debug']:\n",
    "            print(f\"error in get tx as node:\\n{e}\")\n",
    "            \n",
    "    finally:            \n",
    "        return tx_node\n",
    "\n",
    "def getEligius_taint_analysis():\n",
    "    \"\"\"carries out the taint analysis on the Eligius pool for k steps, obtaining the graph of the path of the Bitcoins\n",
    "    \n",
    "    @params : no params\n",
    "\n",
    "    @return : list of nodes of the graph related of Eligius pool taint analysis\n",
    "    \"\"\"\n",
    "\n",
    "    startT = time.time()\n",
    "    if LOG_LEVELS['time']:\n",
    "        print(\"Eligius taint analysis miners started\")\n",
    "    \n",
    "    steps = SETTINGS['ELIGIUS_ANALYSIS_STEPS']\n",
    "    if steps < 0:\n",
    "        raise ValueError('ELIGIUS_ANALYSIS_STEPS must be greater than 0')\n",
    "\n",
    "    nodes = []\n",
    "    last_outputs = []\n",
    "    for step in range(steps):\n",
    "        startLen = len(nodes)\n",
    "        currentOutputs = []\n",
    "        if step == 0:\n",
    "            eligiusCoinbaseNode = getTxAsNode(ELIGIUS_COINBASE_TX)\n",
    "            nodes.append(eligiusCoinbaseNode)        \n",
    "            currentOutputs += eligiusCoinbaseNode['outputs']\n",
    "        else:  \n",
    "            if (not LOG_LEVELS['reduce spam'] or LOG_LEVELS['results']) and (LOG_LEVELS['debug'] or LOG_LEVELS['all infos'] or LOG_LEVELS['processing']):\n",
    "                print(f\"going to proceed {len(last_outputs)} transactions\")\n",
    "                  \n",
    "            max_t_quantity = min(MAX_THREAD_QUANTITY, len(last_outputs))\n",
    "            if max_t_quantity > 7: #reduce the max threads quantity to 7 to do not overload the use of resources\n",
    "                max_t_quantity = 7\n",
    "            with ThreadPoolExecutor(max_workers=max_t_quantity) as executor:\n",
    "                future_to_node = {executor.submit(getTxAsNode, id): id for id in last_outputs}\n",
    "                for future in as_completed(future_to_node):\n",
    "                    node = future.result()\n",
    "                    nodes.append(node)\n",
    "                    currentOutputs += node['outputs']\n",
    "            \n",
    "            \n",
    "                \n",
    "        last_outputs = currentOutputs\n",
    "        \n",
    "        if (not LOG_LEVELS['reduce spam'] or LOG_LEVELS['results']) and (LOG_LEVELS['debug'] or LOG_LEVELS['all infos'] or LOG_LEVELS['processing']):\n",
    "            print(f\"-> found {len(nodes)-startLen} nodes at step {step}/{steps}\")\n",
    "            \n",
    "    if LOG_LEVELS['debug'] or LOG_LEVELS['all infos'] or LOG_LEVELS['results']:\n",
    "        print(f\"Found {len(nodes)} nodes in {steps} steps\")            \n",
    "    \n",
    "    if LOG_LEVELS['time']:\n",
    "        print(f\"Eligius taint analysis miners ended in {time.time()-startT} seconds\")\n",
    "     \n",
    "    return nodes\n",
    "\n",
    "               \n",
    "            \n",
    "if __name__ == \"__main__\": #for module tests \n",
    "    os.system(\"cls\" if os.name == \"nt\" else \"clear\")  # clear console\n",
    "    \n",
    "    pools = getPools()\n",
    "    print(f\"\\npools = {pools}\")\n",
    "    \n",
    "    nodesEligius_T_analisys = getEligius_taint_analysis()\n",
    "    print(f\"nodesEligius_T_analisys:\\n{nodesEligius_T_analisys}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eaacf9-4682-436a-9db7-5ac65b0d064e",
   "metadata": {},
   "source": [
    "## Main \n",
    "\n",
    "**Global variables and take datas from CSV files**\n",
    "\n",
    "In questa prima parte di codice mi limito ad importare le librerie necessarie, a definire alcune variabili globali utili e le funzioni necessarie per leggere correttamente i files CSV.\n",
    "A riguardo ho deciso di implementare un semplice meccanismo di threads (sfruttato dalla funzione takeCSV_data) al fine di velocizzare ulteriormente le operazioni di lettura dei files.\n",
    "Ho anche differenziato la lettura di ogni file con una relativa funzione per poter sfruttare uno schema per i tipi di dati (in modo da ridurre quanto più possibile le dimensioni dei dati letti), per utilizzare unicamente le colonne necessarie e per leggere ogni file in chunk (la cui dimensione viene calcolata da calculate_chunk_size).\n",
    "Con questi accorgimenti son riuscito a ridurre considerevolmente il tempo di lettura dei files fino ad arrivare a circa un minuto per i files transactions, inputs e outputs (assieme) e poco più di un minuto per il file map (che richiede maggior tempo dovendo leggere gli hash).\n",
    "\n",
    "**Dataframe analisi, network congestion, fees e script types**\n",
    "\n",
    "Una volta ottenuti i dataframes relativi ai dati nei files csv inizialmente necessari inizia l'elaborazione delle transazioni per ottenere la network congeston e le fees per intervalli temporali di un mese e per ottenere la quantità di script type sempre per ogni mese.\n",
    "Per far ciò vengono considerate le transazioni non Coinbase che vengono processate sfruttando il modulo dataset analysis ottenendo un dataframe di mesi utilizzabile per la creazione dei grafici.\n",
    "\n",
    "**Scraping e mining pool analisi**\n",
    "\n",
    "Successivamente vengono considerate le transazioni Coinbase, viene creato il dataframe relativo a map.csv, vengono ottenuti gli indirizzi associati alle mining pools tramite il modulo di scraping, vengono filtrate le transazioni per ottenere quelle associate alle mining pools e quelle non associate.\n",
    "In seguito vengono trovati i top quattro miners raggruppando per txHash e contando le transazioni per ogni miner, ordinando per ordine decrescente di blocchi minati e prendendo i primi 4 (gli altri vengono inseriti in un dataframe a parte).\n",
    "Vengono poi calcolate le statistiche richieste : total rewards e numero di blocchi minati per le pools sia globalmente che per intervalli temporali di due mesi e tali statistiche vengono utilizzate per la creazione dei relativi grafici.\n",
    "Infine viene effettuata la taint analisi su Eligius sfruttando il modulo di scraping, creando un dataframe relativo al grafo e producendo il grafico che mostra il percorso dei bitcoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebba20a-1b89-4627-9577-1efa821a3cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "import time\n",
    "from graphic import plot_creator\n",
    "from utilities import calculate_chunk_size\n",
    "from utilities import unix_to_date\n",
    "from dataset_analysis import analizer\n",
    "from scraping import scraper\n",
    "from utilities import LOG_LEVELS, SETTINGS\n",
    "\n",
    "pd.set_option(\"mode.copy_on_write\", True)\n",
    "\n",
    "#csv file paths : \n",
    "CURRENT_PATH = os.path.dirname(__file__)\n",
    "DATASET_ANALYSIS_PATH = os.path.join(CURRENT_PATH, \"dataset_analysis\")\n",
    "DATASET_PATH = os.path.join(DATASET_ANALYSIS_PATH, \"datasetCSV\")\n",
    "\n",
    "INPUTS_CSV_PATH = os.path.join(DATASET_PATH, \"inputs.csv\")\n",
    "MAP_CSV_PATH = os.path.join(DATASET_PATH, \"map.csv\")\n",
    "OUTPUTS_CSV_PATH = os.path.join(DATASET_PATH, \"outputs.csv\")\n",
    "TRANSACTIONS_CSV_PATH = os.path.join(DATASET_PATH, \"transactions.csv\")\n",
    "\n",
    "MAX_THREAD_QUANTITY = SETTINGS['MAX_THREAD_QUANTITY']\n",
    "CHUNK_SIZE = 10000\n",
    "\n",
    "\n",
    "inputs_columns = [\"txId\", \"prevTxId\", \"prevTxpos\"]\n",
    "map_columns = [\"hash\", \"addressId\"]\n",
    "outputs_columns = [\"txId\", \"position\", \"addressId\", \"amount\", \"scripttype\"]\n",
    "transactions_columns = [\"timestamp\", \"blockId\", \"txId\", \"isCoinbase\", \"fee\"]\n",
    "\n",
    "\n",
    "\n",
    "# --- read datas by csv files (for calculation of network congestion and script type data) :\n",
    " \n",
    "def read_csv_chunk(schema, columns, usecols, filePath, chunkSize = CHUNK_SIZE, parseDate = False):                    \n",
    "    \"\"\"Read a CSV by chunks and returns chunks\n",
    "    @param schema : schema of data types (to reduce dimension)\n",
    "    @param columns : column'n names \n",
    "    @param usecols : columns indexes of csv file\n",
    "    @param filePath : path of the CSV file\n",
    "    @param [optional] chunkSize : size of 1 chunk (default = CHUNK_SIZE)\n",
    "    @param [optional] parseDate : boolean to parse timestamp dates (default = False)\n",
    "    @return chunks\n",
    "    \"\"\"\n",
    "    chuncks = pd.read_csv(filePath, usecols=usecols, dtype=schema, names=columns, parse_dates=parseDate, chunksize= chunkSize)           \n",
    "    return chuncks\n",
    "   \n",
    "def readInputs():\n",
    "    \"\"\"Read inputs csv by chunk \n",
    "    (calculating the chunk size using the number of rows in csv file)\n",
    "    @no params\n",
    "    @return inputs dataframe\n",
    "    \"\"\"\n",
    "    startT = time.time()\n",
    "    print('\\nStarted reading inputs csv')\n",
    "    schema = {        \n",
    "        \"txId\": \"int32\",                 \n",
    "    }\n",
    "    usecols = [0]\n",
    "    rowsInFile = 21378771\n",
    "    _ , chunk_size = calculate_chunk_size(rowsInFile, 200000)\n",
    "    columns = [\"txId\"]\n",
    "    chunks = read_csv_chunk(schema,columns,usecols,INPUTS_CSV_PATH,chunk_size)\n",
    "    df = pd.concat(chunks)\n",
    "    df = df.drop_duplicates(subset=['txId'])\n",
    "    endT = time.time()\n",
    "    diff = endT - startT\n",
    "    if LOG_LEVELS['time']:\n",
    "        print(f\"\\nInputs csv read in {diff} seconds\")\n",
    "    return df\n",
    "\n",
    "def readOutputs():\n",
    "    \"\"\"Read outputs csv by chunk \n",
    "    (calculating the chunk size using the number of rows in csv file)\n",
    "    @no params\n",
    "    @return outputs dataframe\n",
    "    \"\"\"\n",
    "    startT = time.time()\n",
    "    print('\\nStarted reading outputs csv ')\n",
    "    schema = {        \n",
    "        \"txId\": \"int32\",                 \n",
    "        \"scriptType\": \"int8\",                 \n",
    "        \"amount\": \"int64\",                 \n",
    "        \"addressId\": \"int32\",                 \n",
    "    }\n",
    "    usecols = [0,2,3,4]\n",
    "    rowsInFile = 24613799\n",
    "    _ , chunk_size = calculate_chunk_size(rowsInFile, 210000)\n",
    "    columns = [\"txId\", \"addressId\",\"amount\", \"scriptType\"]\n",
    "    chunks = read_csv_chunk(schema,columns,usecols,OUTPUTS_CSV_PATH,chunk_size)\n",
    "    df = pd.concat(chunks)\n",
    "    df = df.drop_duplicates(subset=['txId'])\n",
    "    endT = time.time()\n",
    "    diff = endT - startT\n",
    "    if LOG_LEVELS['time']:\n",
    "        print(f\"\\nOutputs csv read in {diff} seconds\")\n",
    "    return df\n",
    "\n",
    "def readTransaction():\n",
    "    \"\"\"Read transactions csv by chunk \n",
    "    (calculating the chunk size using the number of rows in csv file)\n",
    "    @no params\n",
    "    @return outputs dataframe\n",
    "    \"\"\"   \n",
    "    startT = time.time()\n",
    "    print('\\nStarted reading transactions csv')\n",
    "    schema = {\n",
    "        \"timestamp\": \"int32\",\n",
    "        \"blockId\": \"int32\",\n",
    "        \"txId\": \"int32\",\n",
    "        \"isCoinbase\": \"int8\",\n",
    "        \"fee\": \"int32\",                \n",
    "    }\n",
    "    usecols = [0, 1, 2, 3, 4]\n",
    "    \n",
    "    rowsInFile = 10572829\n",
    "    _ , chunk_size = calculate_chunk_size(rowsInFile, 200000)\n",
    "    columns = [\"timestamp\", 'blockId' ,\"txId\", \"isCoinbase\", \"fee\"]    \n",
    "    chunks = read_csv_chunk(schema,columns,usecols,TRANSACTIONS_CSV_PATH,chunk_size)\n",
    "    df = pd.concat(chunks)    \n",
    "    df = df.drop_duplicates(subset=['txId'])    \n",
    "   \n",
    "    df['timestamp'] = df['timestamp'].apply(unix_to_date)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "    \n",
    "    endT = time.time()\n",
    "    diff = endT - startT\n",
    "    if LOG_LEVELS['time']:\n",
    "        print(f\"\\nTransaction csv read in {diff} seconds\")\n",
    "    return df \n",
    "\n",
    "def takeCSV_data():\n",
    "    \"\"\"Take CSV data from inputs, outputs, transactions csv and convert them to dataframes.\n",
    "    Use threads to improve performances.\n",
    "    \n",
    "    @no params \n",
    "    @return inputs, outputs, transactions dataframes\n",
    "    \"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=MAX_THREAD_QUANTITY) as executor:\n",
    "        inputs_future = executor.submit(readInputs)\n",
    "        inputs = inputs_future.result()\n",
    "        \n",
    "    with ThreadPoolExecutor(max_workers=MAX_THREAD_QUANTITY) as executor:\n",
    "        outputs_future = executor.submit(readOutputs)\n",
    "        outputs = outputs_future.result()        \n",
    "        \n",
    "    with ThreadPoolExecutor(max_workers=MAX_THREAD_QUANTITY*3) as executor:\n",
    "        tx_future = executor.submit(readTransaction)        \n",
    "        tx = tx_future.result()       \n",
    "            \n",
    "    return inputs, outputs, tx\n",
    "\n",
    "# --- read datas by csv files (for scraping) :\n",
    "\n",
    "def readMap():\n",
    "    \"\"\"Read map csv by chunk \n",
    "    (calculating the chunk size using the number of rows in csv file)\n",
    "    @no params\n",
    "    @return map dataframe\n",
    "    \"\"\" \n",
    "    startT = time.time()\n",
    "    print('\\nStarted reading map csv')\n",
    "    schema = {        \n",
    "        \"txHash\":'category',\n",
    "        \"addressId\": \"int32\"              \n",
    "    }\n",
    "    usecols = [0,1]\n",
    "    rowsInFile = 8708821\n",
    "    _ , chunk_size = calculate_chunk_size(rowsInFile, 200000)\n",
    "    columns = [\"txHash\",\"addressId\"]\n",
    "    chunks = read_csv_chunk(schema,columns,usecols,MAP_CSV_PATH,chunk_size)\n",
    "    df = pd.concat(chunks)\n",
    "    endT = time.time()\n",
    "    diff = endT - startT\n",
    "    if LOG_LEVELS['time']:\n",
    "        print(f\"\\nMap csv read in {diff} seconds\")\n",
    "    return df\n",
    "        \n",
    "def takeMapCSV_data():  \n",
    "    \"\"\"Take map CSV data from map csv and convert them to dataframes.\n",
    "    Use threads to improve performances.\n",
    "    \n",
    "    @no params \n",
    "    @return map dataframes\n",
    "    \"\"\" \n",
    "    with ThreadPoolExecutor(max_workers=MAX_THREAD_QUANTITY*3) as executor:        \n",
    "        map_future = executor.submit(readMap)                        \n",
    "        mapDF = map_future.result()       \n",
    "                    \n",
    "    return mapDF\n",
    "\n",
    "# --- main : \n",
    "\n",
    "def main():\n",
    "    main_execution_start_time = time.time()\n",
    "    \n",
    "    # --- read input, output, transactions csv and create DF\n",
    "    input_dataframe, outputs_dataframe, transaction_dataframe = takeCSV_data() \n",
    "    line = \"------------------------\"\n",
    "        \n",
    "    if LOG_LEVELS['time']:\n",
    "        print(f\"\\nTime for read csv = {time.time()-main_execution_start_time} seconds\")\n",
    "    \n",
    "    if LOG_LEVELS['debug'] or LOG_LEVELS['all infos']:\n",
    "        print(f\"\\n{line}Dataframes:\\n\\ninputs for congestion:\\n{input_dataframe}\\n{line}\\noutputs for congestion:\\n{outputs_dataframe}\\n{line}\\ntransaction:\\n{transaction_dataframe}\\n{line}\")\n",
    "    \n",
    "    # --- obtain datas of network congestion & script type per month \n",
    "    notCoinbaseTX = transaction_dataframe.loc[transaction_dataframe['isCoinbase'] != 1] \n",
    "    notCoinbaseTX = notCoinbaseTX.drop('blockId', axis=1)\n",
    "    notCoinbaseTX.set_index('timestamp',inplace=True)\n",
    "    outputsToCalculateCongestion = outputs_dataframe.drop('addressId', axis=1)\n",
    "    month_data_DF = analizer.processTransactions(input_dataframe, outputsToCalculateCongestion, notCoinbaseTX)\n",
    "    \n",
    "    # --- create plots for stats (network congestion & fees | script type)\n",
    "    plot_creator.plot_fees_vs_network_congestion(month_data_DF)\n",
    "    plot_creator.plot_script_type_usage(month_data_DF)\n",
    "    plot_creator.plot_annual_script_type_usage(month_data_DF)\n",
    "    \n",
    "    # --- scraping & mining pool analysis :\n",
    "    coinbaseTX = transaction_dataframe.loc[transaction_dataframe['isCoinbase'] == 1] #filter coinbase tx\n",
    "    coinbaseTX = coinbaseTX.drop('fee', axis=1) # delete fee column\n",
    "    coinbaseTX = coinbaseTX.drop_duplicates(subset=['txId']) #delete any duplicates\n",
    "    if LOG_LEVELS['debug'] or LOG_LEVELS['all infos']:\n",
    "        print(f\"coinbaseTX:{coinbaseTX}\")\n",
    "    \n",
    "    outputsToMiningPoolsAnalisis = outputs_dataframe.drop('scriptType', axis=1) #delete script type \n",
    "    outputsToMiningPoolsAnalisis = outputsToMiningPoolsAnalisis.drop_duplicates(subset=['txId']) #delete any duplicates\n",
    "    \n",
    "    if LOG_LEVELS['debug'] or LOG_LEVELS['all infos']:\n",
    "        print(f\"outputs To Mining Pools Analisis:\\n{outputsToMiningPoolsAnalisis}\")\n",
    "    \n",
    "    mapDF = takeMapCSV_data() #take map dataframe\n",
    "    if LOG_LEVELS['debug'] or LOG_LEVELS['all infos']:\n",
    "        print(f\"mapDF:\\n\\n{mapDF}\")\n",
    "    \n",
    "    \n",
    "    parsedTxDF = pd.merge(coinbaseTX, outputsToMiningPoolsAnalisis, on='txId') #merge coinbase tx with (parsed) outputs \n",
    "    if LOG_LEVELS['debug'] or LOG_LEVELS['all infos']:\n",
    "        print(f\"parsedTxDF (1):\\n\\n{parsedTxDF}\")\n",
    "    \n",
    "    parsedTxDF = pd.merge(parsedTxDF, mapDF, on='addressId') #merge previous resoult with map dataframe (to insert hash column)\n",
    "    if LOG_LEVELS['debug'] or LOG_LEVELS['all infos']:\n",
    "        print(f\"parsedTxDF (2):\\n\\n{parsedTxDF}\")\n",
    "    \n",
    "    \n",
    "    miningPoolAddressesDF = scraper.getPools() #get dataframe with txHash<-->mining pool association\n",
    "    if LOG_LEVELS['debug'] or LOG_LEVELS['all infos']:\n",
    "        print(f'\\nminingPoolAddressesDF:\\n{miningPoolAddressesDF}')\n",
    "    \n",
    "    #merge previous resoult with parsedTxDF to obtain all coinbase tx associated to a mining pool :\n",
    "    # -> included coinbase tx that have addresses that do not belong to any of the 4 mining pools\n",
    "    allCoinbaseTxWithPools = pd.merge(parsedTxDF,miningPoolAddressesDF, on='txHash', how='left') \n",
    "    if LOG_LEVELS['debug'] or LOG_LEVELS['all infos']:\n",
    "        print(f'\\nallCoinbaseTxWithPools:\\n{allCoinbaseTxWithPools}')\n",
    "  \n",
    "\n",
    "    #filter transactions that not have the address associated to any mining pool\n",
    "    coinbaseNotAssociated = allCoinbaseTxWithPools[allCoinbaseTxWithPools['pool'].isna()]\n",
    "    coinbaseNotAssociated.drop('pool', axis=1)\n",
    "    if LOG_LEVELS['debug'] or LOG_LEVELS['all infos']:\n",
    "        print(f'\\ncoinbaseNotAssociated:\\n{coinbaseNotAssociated}')\n",
    "    \n",
    "    #filter transactions that have address associated to a mining pool\n",
    "    coinbase_associated = allCoinbaseTxWithPools[~allCoinbaseTxWithPools['pool'].isna()]\n",
    "    if LOG_LEVELS['debug'] or LOG_LEVELS['all infos']:\n",
    "        print(f'\\ncoinbase_associated:\\n{coinbase_associated}')\n",
    "\n",
    "        \n",
    "    #group by 'txHash' and count transactions for each miner\n",
    "    miner_counts = coinbaseNotAssociated.groupby('addressId')['txHash'].count().reset_index(name='blocks_mined')\n",
    "    \n",
    "    #sort miners     \n",
    "    sorted_miners = miner_counts.sort_values(by='blocks_mined', ascending=False)\n",
    "\n",
    "    #select top 4 miners \n",
    "    top_4_miners = sorted_miners.head(4)\n",
    "\n",
    "    if LOG_LEVELS.get('debug', False):\n",
    "        print(f\"Top 4 miners:\\n{top_4_miners}\")\n",
    "        \n",
    "    \n",
    "    others_miners = sorted_miners.iloc[4:]\n",
    "    if LOG_LEVELS.get('debug', False):\n",
    "        print(f\"others miners:\\n{others_miners}\")\n",
    "\n",
    "        \n",
    "  \n",
    "    # Calculate global statistics\n",
    "    global_blocks_mined, global_total_rewards = analizer.calculate_pool_statistics(coinbase_associated)\n",
    "    if LOG_LEVELS['debug'] or LOG_LEVELS['all infos']:\n",
    "        print(f'\\nglobal_blocks_mined:\\n{global_blocks_mined}')\n",
    "        print(f'\\nglobal_total_rewards:\\n{global_total_rewards}')\n",
    "\n",
    "    # Calculate bi-monthly statistics\n",
    "    bi_monthly_blocks_mined, bi_monthly_total_rewards = analizer.calculate_bi_monthly_statistics(coinbase_associated)\n",
    "    if LOG_LEVELS['debug'] or LOG_LEVELS['all infos']:\n",
    "        print(f'\\npbi_monthly_blocks_mined:\\n{bi_monthly_blocks_mined}')\n",
    "        print(f'\\nbi_monthly_total_rewards:\\n{bi_monthly_total_rewards}')\n",
    "    \n",
    "    # Plot statistics\n",
    "    plot_creator.plot_blocks_mined_by_top_4_miners(top_4_miners)\n",
    "    plot_creator.plot_total_blocks_mined(global_blocks_mined)\n",
    "    plot_creator.plot_bi_monthly_blocks_mined(bi_monthly_blocks_mined)\n",
    "    plot_creator.plot_total_rewards(global_total_rewards)\n",
    "    plot_creator.plot_bi_monthly_rewards(bi_monthly_total_rewards)\n",
    "\n",
    "    #Eligius taint analysis\n",
    "    nodes = scraper.getEligius_taint_analysis()    \n",
    "    if LOG_LEVELS['debug'] or LOG_LEVELS['all infos']:\n",
    "        print(f'\\nnodes:\\n{nodes}')\n",
    "    \n",
    "    graph_DF = pd.DataFrame(nodes)\n",
    "    print(graph_DF)\n",
    "    \n",
    "    plot_creator.plot_Eligius_path(graph_DF)\n",
    "    \n",
    "        \n",
    "def test_eligius_graph():\n",
    "    #Eligius taint analysis\n",
    "    nodes = scraper.getEligius_taint_analysis()    \n",
    "    \n",
    "    graph_DF = pd.DataFrame(nodes)\n",
    "    print(graph_DF)\n",
    "    \n",
    "    plot_creator.plot_Eligius_path(graph_DF)\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.system(\"cls\" if os.name == \"nt\" else \"clear\")  # clear console\n",
    "    execution_start_time = time.time()\n",
    "    main()\n",
    "    #test_eligius_graph()\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - execution_start_time\n",
    "    if LOG_LEVELS['time']:\n",
    "        print(f\"\\nTotal execution time: {elapsed_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140ed681-547a-457f-a141-2e3f45dc3263",
   "metadata": {},
   "source": [
    "## Considerazioni finali \n",
    "\n",
    "Il progetto è stato consegnato in formato .pdf, tuttavia allego il link della "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
